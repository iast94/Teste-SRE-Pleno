# Teste-SRE-Pleno

## üöÄ Introdu√ß√£o
Este projeto utiliza automa√ß√£o total. Para implantar a solu√ß√£o:

1. **Configura√ß√£o:** Adicione os `Secrets` necess√°rios no seu reposit√≥rio GitHub (veja a se√ß√£o de Guia de Instala√ß√£o).
2. **Deploy:** Realize um `git push` para a branch `main`.
3. **Monitoramento:** O pipeline far√° o build, push e deploy via Helm automaticamente no cluster configurado.

## üèó Arquitetura
A solu√ß√£o consiste em uma aplica√ß√£o Node.js containerizada, rodando em um cluster Kubernetes ((**k3s**) gerenciado via **k3d**. A arquitetura inclui suporte para auto-scaling (HPA), monitoramento via Prometheus/Grafana e agrega√ß√£o de logs via ELK Stack.

## ‚öñÔ∏è Por que k3d/k3s? (Decis√£o de Infraestrutura)
Diferente de ferramentas como **Kind** (Kubernetes in Docker) ou **Minikube**, a escolha pelo **k3d/k3s** para este projeto baseia-se em:

* **Leveza e Performance:** O k3s √© um bin√°rio √∫nico de < 100MB que consome significativamente menos mem√≥ria RAM que o Minikube, sendo ideal para ambientes de ef√™meros e execu√ß√µes via Self-hosted Runners.
* **Pronto para Produ√ß√£o:** Enquanto o Kind √© focado estritamente em testes locais de CI, o k3s √© uma distribui√ß√£o certificada pela CNCF pronta para uso em produ√ß√£o, o que aproxima este laborat√≥rio de um cen√°rio real.
* **Simplicidade Operacional via k3d:** O uso do **k3d** permite subir o cluster k3s rapidamente como containers Docker, eliminando o overhead de gerenciar m√°quinas virtuais pesadas (Minikube) ou drivers de virtualiza√ß√£o complexos, mantendo compatibilidade total com Helm e manifestos padr√£o.
* **Integra√ß√£o com CI/CD Local:** A arquitetura baseada em Docker facilitou a integra√ß√£o com o **GitHub Self-hosted Runner**, permitindo que o pipeline de CD acesse o API Server do cluster sem a necessidade de expor portas para a internet p√∫blica.

## üõ† Componentes
* **App:** Microservi√ßo em Node.js com suporte a health checks e exporta√ß√£o de m√©tricas.
* **K8S:** Cluster multi-node com Deployment, Service, HPA, PDB e ConfigMaps.
* **CI/CD:** Pipeline automatizado via GitHub Actions para Build e Deploy (Docker Hub + Helm).
* **Observabilidade:** Stack Prometheus e ELK integrados.

## üõ†Ô∏è Guia de Instala√ß√£o

Este projeto foi desenhado para ser totalmente port√°til via **Infrastructure as a Template**.

### 1. Configura√ß√£o de Secrets no GitHub
Para o funcionamento do pipeline, configure os seguintes Segredos em seu reposit√≥rio (**Settings > Secrets and variables > Actions > New repository secret**):

| Nome do Secret | Descri√ß√£o |
| :--- | :--- |
| `DOCKERHUB_USERNAME` | Seu nome de usu√°rio no Docker Hub. |
| `DOCKERHUB_TOKEN` | Seu Personal Access Token do Docker Hub. |
| `KUBE_CONFIG_DATA` | O conte√∫do do seu arquivo `~/.kube/config` em Base64. |

### 2. Como gerar o Token do Docker Hub
Para maior seguran√ßa, utilize um Personal Access Token (PAT) em vez da sua senha:
1. No Docker Hub, v√° em **Account Settings > Security > Generate new token**.
2. Gere um token com permiss√µes de `Read & Write`.
3. Use este token no secret `DOCKERHUB_TOKEN`.

### 3. Como exportar o KUBECONFIG
O pipeline utiliza o arquivo de configura√ß√£o para autentica√ß√£o externa.

1. No terminal onde o `kubectl` est√° configurado, execute:
   ```bash
   cat ~/.kube/config | base64 -w 0
2. Copie toda a string resultante.
3. No GitHub, cole este valor no secret `KUBE_CONFIG_DATA`.

### 4. Execu√ß√£o
Qualquer altera√ß√£o enviada para a branch `main` disparar√° o workflow `.github/workflows/main.yml`. Este pipeline gerencia:
* **Build e Push:** Envio da imagem para o Docker Hub.
* **Deploy:** Cria√ß√£o do namespace e instala√ß√£o via Helm no Kubernetes.

## üê≥Tarefa 1: Containeriza√ß√£o & Execu√ß√£o - Decis√µes T√©cnicas: Dockerfile

### 1. Imagem Base: Node 20-alpine (Active LTS)
* **Escolha:** Foi utilizada a vers√£o `node:20-alpine`.
* **Justificativa de Tamanho:** O Alpine Linux √© uma distribui√ß√£o minimalista reduzindo assim o tempo de download (pull) e o consumo de storage no cluster.
* **Justificativa de Seguran√ßa:** Por conter apenas o essencial para a execu√ß√£o do SO, o Alpine possui menos bin√°rios e bibliotecas instaladas. Isso reduz drasticamente a "superf√≠cie de ataque", diminuindo o n√∫mero de vulnerabilidades (CVEs) potenciais que ferramentas de scan podem encontrar.

### 2. Otimiza√ß√£o de Build: Multi-Stage, Camadas e Cache
* **Aproveitamento de Cache:** A c√≥pia dos arquivos `package.json` e `yarn.lock` foi realizada antes da c√≥pia do restante do c√≥digo fonte. Como o Docker funciona em camadas (layers), isso garante que, se o c√≥digo mudar mas as depend√™ncias n√£o, o Docker reutilize a camada de instala√ß√£o (cache), acelerando o tempo de build no pipeline CI/CD.
* **Redu√ß√£o de Camadas:**  O arquivo foi organizado para agrupar comandos RUN, reduzindo o n√∫mero de camadas intermedi√°rias e o tamanho final da imagem.
* **Multi-Stage Build:** Foi implementada a separa√ß√£o entre o est√°gio de constru√ß√£o (build) e o de execu√ß√£o (runtime). O ambiente final cont√©m apenas os artefatos compilados, eliminando compiladores e arquivos fonte, o que garante uma imagem mais leve e segura para o ambiente de stagin.

### 3. Gerenciamento de Depend√™ncias com Yarn
* **Determinismo com yarn.lock:** A inclus√£o do arquivo yarn.lock no reposit√≥rio e no build garante que as vers√µes das bibliotecas sejam exatamente as mesmas em qualquer ambiente (Local, CI/CD e Produ√ß√£o).
* **Flag --frozen-lockfile:** Garante que o Yarn n√£o tente atualizar o arquivo lock durante o build. Se houver discrep√¢ncia, o build falha, evitando comportamentos inesperados.
* **Flag --production:** Instalamos apenas as dependencies essenciais para rodar o app. Depend√™ncias de desenvolvimento (devDependencies), como linters ou frameworks de teste, s√£o ignoradas para reduzir a superf√≠cie de ataque e o tamanho da imagem.

### 4. Seguran√ßa: Usu√°rio Non-Root com ID Fixo
* **Justificativa do ID 1001:** O uso de um UID/GID fixo acima de 1000 √© uma conven√ß√£o de seguran√ßa para garantir que o usu√°rio da aplica√ß√£o n√£o coincida com usu√°rios do sistema host (como o root, que √© ID 0). Al√©m disso, IDs fixos facilitam a gest√£o de permiss√µes de volumes (RBAC) e pol√≠ticas de seguran√ßa do pod (PodSecurityPolicies) no Kubernetes.
* **Privil√©gios M√≠nimos:** Rodar o processo como non-root impede que, em caso de invas√£o da aplica√ß√£o, o atacante obtenha privil√©gios administrativos sobre o kernel do n√≥ hospedeiro.
* **ID 1001 vs appuser:** Em vez de apenas usar um nome como appuser, definir explicitamente o UID 1001 √© uma boa pr√°tica porque muitos sistemas de arquivos e ferramentas de seguran√ßa monitoram o ID num√©rico.
* **Minimiza√ß√£o de Ferramentas:** Ao usar --production, removemos ferramentas de build que poderiam ser exploradas por atacantes dentro do container.

### 5. Execu√ß√£o: Bin√°rio Direto vs Gerenciadores
* **Comando:** Foi definido o uso de `CMD ["node", "src/app.js"]`.
* **Sinais do Sistema:** O Node.js foi configurado como o processo principal (PID 1) para que possa receber sinais de termina√ß√£o do Kubernetes, como o `SIGTERM`. Gerenciadores como `npm` ou `yarn` costumam "encapsular" o processo, impedindo que os sinais cheguem ao Node, o que inviabilizaria um Graceful Shutdown (desligamento limpo).
* **Determinismo:** O uso do par√¢metro `--frozen-lockfile` no build garante que as vers√µes das depend√™ncias instaladas sejam exatamente as testadas, evitando desvios entre ambientes.

## ‚ò∏Ô∏è Tarefa 2: Deployment Kubernetes - Decis√µes T√©cnicas: Helm & Kubernetes

A arquitetura de deployment foi projetada para garantir alta disponibilidade, escalabilidade autom√°tica e isolamento de recursos, seguindo as melhores pr√°ticas de infraestrutura como c√≥digo.

#### 1. Orquestra√ß√£o Declarativa e Reutiliza√ß√£o (Helm & Helmfile)
* **Orquestra√ß√£o via Helmfile:** A utiliza√ß√£o do Helmfile permite gerenciar o ciclo de vida da aplica√ß√£o e da stack de observabilidade de forma unificada. Atrav√©s da defini√ß√£o de depend√™ncias (`needs`), garante-se que o monitoramento esteja operacional antes do deploy da aplica√ß√£o principal.
* **Abstra√ß√£o via Values:** Todos os par√¢metros sens√≠veis e de configura√ß√£o (portas, caminhos de health check, limites de recursos) foram movidos para o arquivo `values.yaml`. Isso permite que o mesmo chart seja utilizado em diferentes ambientes apenas alterando o arquivo de valores, sem a necessidade de modificar os templates base.
* **Uso de Helpers:** Foi implementado o arquivo `_helpers.tpl` para gerenciar a nomenclatura dos recursos e labels de forma din√¢mica. O uso da fun√ß√£o `fullname` garante a unicidade dos nomes dentro do cluster, evitando colis√µes de recursos entre diferentes releases.

### 2. Alta Disponibilidade e Distribui√ß√£o (Topology Spread Constraints)
* **Estrat√©gia de Espalhamento:** Foi utilizada a funcionalidade de `topologySpreadConstraints` com `maxSkew: 1` e `topologyKey: kubernetes.io/hostname`. 
* **Justificativa:** Diferente de uma afinidade simples, o Spread Constraint garante matematicamente que as r√©plicas da aplica√ß√£o sejam distribu√≠das de forma equilibrada entre os n√≥s dispon√≠veis (`node-01` e `node-02`). O uso de `whenUnsatisfiable: DoNotSchedule` assegura que o cluster n√£o concentre pods em um √∫nico n√≥, mitigando o risco de downtime total em caso de falha de um host f√≠sico.

### 3. Resili√™ncia e Ciclo de Vida (PDB e Probes)
* **Pod Disruption Budget (PDB):** Foi implementado um PDB com `minAvailable: 1`. Esta configura√ß√£o √© vital para opera√ß√µes de SRE, pois impede que manuten√ß√µes automatizadas (como o dreno de um n√≥) desliguem todas as inst√¢ncias da aplica√ß√£o simultaneamente, garantindo que pelo menos 50% da capacidade esteja sempre ativa.
* **Startup Probe:** Implementada para proteger o processo de inicializa√ß√£o da aplica√ß√£o, permitindo um tempo de car√™ncia maior para o carregamento de depend√™ncias antes que as probes de integridade iniciem suas verifica√ß√µes.
* **Health Checks Din√¢micos:** As Probes de `liveness` e `readiness` foram parametrizadas para validar a sa√∫de da aplica√ß√£o em tempo real. A separa√ß√£o entre liveness (rein√≠cio do container) e readiness (entrada no balanceador) garante que o tr√°fego s√≥ seja direcionado para pods que completaram seu processo de inicializa√ß√£o.

### 4. Escalabilidade Autom√°tica (HPA v2)
* **M√©tricas Combinadas:** O Horizontal Pod Autoscaler foi configurado para monitorar tanto CPU quanto Mem√≥ria simultaneamente.
* **Thresholds de Performance:** Foram definidos gatilhos de **70% para CPU** e **75% para Mem√≥ria**, conforme requisitos t√©cnicos do projeto. Esta abordagem h√≠brida protege a aplica√ß√£o contra gargalos de processamento e vazamentos de mem√≥ria (memory leaks), garantindo que o cluster escale horizontalmente de forma proativa antes da degrada√ß√£o da lat√™ncia.

### 5. Estrat√©gia de Deploy (Rolling Update)
* **Zero Downtime:** Foi configurada a estrat√©gia `RollingUpdate` com `maxUnavailable: 0`. Isso garante que o Kubernetes nunca remova uma vers√£o antiga da aplica√ß√£o sem antes ter uma nova vers√£o saud√°vel e pronta para receber tr√°fego, eliminando quedas de servi√ßo durante atualiza√ß√µes de vers√£o.
* **Justificativa:** Esta escolha garante que a capacidade total da aplica√ß√£o (2 r√©plicas) seja preservada durante todo o processo de atualiza√ß√£o. O Kubernetes √© for√ßado a instanciar um novo Pod saud√°vel antes de iniciar o encerramento de qualquer inst√¢ncia da vers√£o anterior, evitando gargalos de processamento durante janelas de deploy.

## üöÄ Tarefa 4: Pipeline CI/CD - Decis√µes T√©cnicas: CI/CD (GitHub Actions)

A automa√ß√£o do ciclo de vida da aplica√ß√£o foi implementada via GitHub Actions, focando em garantir a integridade do c√≥digo e a consist√™ncia dos deploys.

### 1. Estrat√©gia de Runner: GitHub Self-hosted
* **Conectividade Local:** Como o cluster Kubernetes (k3d) est√° rodando localmente, foi utilizado um runner self-hosted para permitir que o pipeline alcance o Control Plane do cluster sem a necessidade de expor APIs para a internet p√∫blica (via Ngrok ou t√∫neis), mantendo a comunica√ß√£o restrita ao ambiente interno.
* **Seguran√ßa de Rede:** O uso do runner local elimina vetores de ataque externos, mantendo toda a comunica√ß√£o de deploy restrita ao ambiente controlado.
* **Efici√™ncia de Cache:** O runner compartilha o daemon do Docker da m√°quina host, permitindo o reaproveitamento imediato de cache de camadas (layers), o que reduz drasticamente o tempo de build das imagens em compara√ß√£o a runners ef√™meros na nuvem.
* **Persist√™ncia:** Diferente de ambientes de laborat√≥rio tempor√°rios, o runner local garante a persist√™ncia das configura√ß√µes de context do kubectl e do Helm, tornando o ciclo de desenvolvimento e deploy mais √°gil e previs√≠vel.

### 2. Pipeline de Integra√ß√£o Cont√≠nua (CI)
* **Uso de `uses` para Tarefas Utilit√°rias:** Foi adotado o padr√£o `uses` para etapas como `actions/checkout` e `docker/setup-buildx-action`. Esta escolha justifica-se pela confiabilidade de Actions oficiais que simplificam tarefas complexas de autentica√ß√£o e setup.
* **Build Multi-arquitetura:** O pipeline realiza o build da imagem Docker utilizando o contexto do Dockerfile multi-stage, garantindo que apenas imagens que passaram nos testes de build avancem para a etapa de push.
* **Versionamento de Imagem:** Foi adotada a estrat√©gia de versionamento via tags num√©ricas incrementais baseadas no `${{ github.run_number }}`. Esta abordagem garante que cada build gere uma vers√£o √∫nica, leg√≠vel e sequencial (ex: `1.10`, `1.11`), facilitando o rastreio de deploys, a gest√£o de imagens no Docker Hub e o rollbacks.

### 3. Pipeline de Entrega Cont√≠nua (CD)
* **Helm Lint:** Antes de qualquer altera√ß√£o no cluster, o pipeline executa o `helm lint` para validar a sintaxe e as boas pr√°ticas dos templates do Chart, evitando falhas de deploy por erros de indenta√ß√£o ou l√≥gica de template.
* **Uso de `run` para Deploys Cr√≠ticos:** Para as etapas de deploy (Helmfile), foi priorizado o uso de comandos `run`. Esta decis√£o t√©cnica visa o controle total sobre a infraestrutura de CI/CD, permitindo maior visibilidade de debug e evitando a depend√™ncia de "caixas-pretas" de terceiros no caminho cr√≠tico de produ√ß√£o.
* **Instala√ß√£o via Script de Bootstrap:** A instala√ß√£o do Helmfile no Runner foi realizada atrav√©s de um script de shell customizado. Esta pr√°tica permite o versionamento da l√≥gica de instala√ß√£o, garante que a mesma vers√£o do bin√°rio seja utilizada em qualquer ambiente e facilita a testabilidade local do processo de setup.
* **Orquestra√ß√£o Declarativa:** Foi utilizado o **Helmfile** para gerenciar a aplica√ß√£o e suas depend√™ncias de forma declarativa, permitindo aplicar toda a stack com um √∫nico comando `helmfile apply`.
* **Idempot√™ncia e Sincroniza√ß√£o:** O Helmfile garante que o cluster reflita exatamente o estado definido nos arquivos de configura√ß√£o, tratando atualiza√ß√µes e instala√ß√µes de forma nativa e segura.
* **Abstra√ß√£o de Ambientes:** O uso do Helmfile facilita a separa√ß√£o de contextos, permitindo que o mesmo pipeline gerencie diferentes estados do cluster de forma organizada.
* **Imutabilidade de Deploy:** O pipeline injeta a tag espec√≠fica do build diretamente no manifesto do Kubernetes via Helm durante o deploy. Isso assegura que o cluster execute exatamente a vers√£o de artefato gerada no ciclo de CI, eliminando a ambiguidade de vers√µes.

### 4. Seguran√ßa e Portabilidade (Secrets Management)
* **Kubeconfig as a Secret:** A autentica√ß√£o com o cluster Kubernetes √© realizada atrav√©s da vari√°vel de ambiente `KUBECONFIG` armazenada nos GitHub Secrets. 
* **Justificativa:** Esta abordagem desacopla o pipeline da infraestrutura subjacente, permitindo que a estrat√©gia de deploy seja reutilizada em qualquer provedor de nuvem ou ambiente on-premises sem altera√ß√µes no c√≥digo. Al√©m disso, garante que credenciais sens√≠veis nunca fiquem expostas no reposit√≥rio.

### 5. Gest√£o de Imagens e Registro Externo (Docker Hub)
* **External Registry:** Foi adotado o Docker Hub como registro oficial de imagens da solu√ß√£o, em detrimento do registro ef√™mero local. 
* **Justificativa:** O uso de um registro externo garante a persist√™ncia dos artefatos de build independentemente da vida √∫til do cluster de teste. Isso facilita auditorias de seguran√ßa externas e permite que a imagem seja testada em m√∫ltiplos ambientes (Hybrid Cloud) sem necessidade de re-build.
* **Autentica√ß√£o Segura:** O acesso ao Docker Hub √© realizado via Personal Access Tokens (PAT) injetados como segredos no GitHub Actions, evitando a exposi√ß√£o de senhas globais da conta.

### 6. Portabilidade e Abstra√ß√£o do Pipeline
* **Generic Workflow:** O pipeline foi projetado para ser agn√≥stico. Todas as refer√™ncias a nomes de registro, tags, vari√°veis, secrets e contextos de infraestrutura foram movidas para GitHub Secrets.
* **Justificativa:** Isso permite que o projeto seja replicado por qualquer outro profissional apenas configurando seus pr√≥prios Segredos (Secrets), sem a necessidade de alterar uma √∫nica linha de c√≥digo nos arquivos YAML ou Helm. Esta abordagem segue o princ√≠pio de "Infrastructure as a Template".