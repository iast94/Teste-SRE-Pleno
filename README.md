# Teste-SRE-Pleno

## üöÄ Introdu√ß√£o
Este projeto utiliza automa√ß√£o total. Para implantar a solu√ß√£o:

1. **Configura√ß√£o:** Adicione os `Secrets` necess√°rios no seu reposit√≥rio GitHub (veja a se√ß√£o de Guia de Instala√ß√£o).
2. **Deploy:** Realize um `git push` para a branch `main`.
3. **Monitoramento:** O pipeline far√° o build, push e deploy via Helm automaticamente no cluster configurado.

## üèó Arquitetura
A solu√ß√£o consiste em uma aplica√ß√£o Node.js containerizada, rodando em um cluster Kubernetes (k3s) com auto-scaling (HPA), monitoramento via Prometheus/Grafana e agrega√ß√£o de logs via ELK Stack.

## ‚öñÔ∏è Por que k3s? (Decis√£o de Infraestrutura)
Diferente de ferramentas como **Kind** (Kubernetes in Docker) ou **Minikube**, a escolha pelo **k3s** para este projeto baseia-se em:

* **Leveza e Performance:** O k3s √© um bin√°rio √∫nico de < 100MB que consome significativamente menos mem√≥ria RAM que o Minikube, sendo ideal para ambientes de SRE/DevOps ef√™meros.
* **Pronto para Produ√ß√£o:** Enquanto o Kind √© focado estritamente em testes locais de CI, o k3s √© uma distribui√ß√£o certificada pela CNCF pronta para uso em produ√ß√£o, o que aproxima este laborat√≥rio de um cen√°rio real.
* **Simplicidade Operacional:** O k3s remove drivers legados e cloud providers desnecess√°rios, mas mant√©m suporte total a Helm e Manifestos padr√£o, facilitando a portabilidade sem o overhead de gerenciar m√°quinas virtuais (Minikube) ou containers Docker aninhados (Kind).

## üõ† Componentes
* **App:** Microservi√ßo em Node.js com suporte a health checks e exporta√ß√£o de m√©tricas.
* **K8S:** Cluster multi-node com Deployment, Service, HPA, PDB e ConfigMaps.
* **CI/CD:** Pipeline automatizado via GitHub Actions para Build e Deploy (Docker Hub + Helm).
* **Observabilidade:** Stack Prometheus e ELK integrados.

## üõ†Ô∏è Guia de Instala√ß√£o

Este projeto foi desenhado para ser totalmente port√°til via **Infrastructure as a Template**.

### 1. Configura√ß√£o de Secrets no GitHub
Para o funcionamento do pipeline, configure os seguintes Segredos em seu reposit√≥rio (**Settings > Secrets and variables > Actions > New repository secret**):

| Nome do Secret | Descri√ß√£o |
| :--- | :--- |
| `DOCKERHUB_USERNAME` | Seu nome de usu√°rio no Docker Hub. |
| `DOCKERHUB_TOKEN` | Seu Personal Access Token do Docker Hub. |
| `KUBE_CONFIG_DATA` | O conte√∫do do seu arquivo `~/.kube/config` em Base64. |

### 2. Como gerar o Token do Docker Hub
Para maior seguran√ßa, utilize um Personal Access Token (PAT) em vez da sua senha:
1. No Docker Hub, v√° em **Account Settings > Security > Generate new token**.
2. Gere um token com permiss√µes de `Read & Write`.
3. Use este token no secret `DOCKERHUB_TOKEN`.

### 3. Como exportar o KUBECONFIG
O pipeline utiliza o arquivo de configura√ß√£o para autentica√ß√£o externa.

1. No terminal onde o `kubectl` est√° configurado, execute:
   ```bash
   cat ~/.kube/config | base64 -w 0
2. Copie toda a string resultante.
3. No GitHub, cole este valor no secret `KUBE_CONFIG_DATA`.

### 4. Execu√ß√£o
Qualquer altera√ß√£o enviada para a branch `main` disparar√° o workflow `.github/workflows/main.yml`. Este pipeline gerencia:
* **Build e Push:** Envio da imagem para o Docker Hub.
* **Deploy:** Cria√ß√£o do namespace e instala√ß√£o via Helm no Kubernetes.

## üê≥Tarefa 1: Containeriza√ß√£o & Execu√ß√£o - Decis√µes T√©cnicas: Dockerfile

### 1. Imagem Base: Node 20-alpine (Active LTS)
* **Escolha:** Foi utilizada a vers√£o `node:20-alpine`.
* **Justificativa de Tamanho:** O Alpine Linux √© uma distribui√ß√£o minimalista reduzindo assim o tempo de download (pull) e o consumo de storage no cluster.
* **Justificativa de Seguran√ßa:** Por conter apenas o essencial para a execu√ß√£o do SO, o Alpine possui menos bin√°rios e bibliotecas instaladas. Isso reduz drasticamente a "superf√≠cie de ataque", diminuindo o n√∫mero de vulnerabilidades (CVEs) potenciais que ferramentas de scan podem encontrar.

### 2. Otimiza√ß√£o de Build: Multi-Stage, Camadas e Cache
* **Aproveitamento de Cache:** A c√≥pia dos arquivos `package.json` e `yarn.lock` foi realizada antes da c√≥pia do restante do c√≥digo fonte. Como o Docker funciona em camadas (layers), isso garante que, se o c√≥digo mudar mas as depend√™ncias n√£o, o Docker reutilize a camada de instala√ß√£o (cache), acelerando o tempo de build no pipeline CI/CD.
* **Redu√ß√£o de Camadas:**  O arquivo foi organizado para agrupar comandos RUN, reduzindo o n√∫mero de camadas intermedi√°rias e o tamanho final da imagem.
* **Multi-Stage Build:** Foi implementada a separa√ß√£o entre o est√°gio de constru√ß√£o (build) e o de execu√ß√£o (runtime). O ambiente final cont√©m apenas os artefatos compilados, eliminando compiladores e arquivos fonte, o que garante uma imagem mais leve e segura para o ambiente de stagin.

### 3. Gerenciamento de Depend√™ncias com Yarn
* **Determinismo com yarn.lock:** A inclus√£o do arquivo yarn.lock no reposit√≥rio e no build garante que as vers√µes das bibliotecas sejam exatamente as mesmas em qualquer ambiente (Local, CI/CD e Produ√ß√£o).
* **Flag --frozen-lockfile:** Garante que o Yarn n√£o tente atualizar o arquivo lock durante o build. Se houver discrep√¢ncia, o build falha, evitando comportamentos inesperados.
* **Flag --production:** Instalamos apenas as dependencies essenciais para rodar o app. Depend√™ncias de desenvolvimento (devDependencies), como linters ou frameworks de teste, s√£o ignoradas para reduzir a superf√≠cie de ataque e o tamanho da imagem.

### 4. Seguran√ßa: Usu√°rio Non-Root com ID Fixo
* **Justificativa do ID 1001:** O uso de um UID/GID fixo acima de 1000 √© uma conven√ß√£o de seguran√ßa para garantir que o usu√°rio da aplica√ß√£o n√£o coincida com usu√°rios do sistema host (como o root, que √© ID 0). Al√©m disso, IDs fixos facilitam a gest√£o de permiss√µes de volumes (RBAC) e pol√≠ticas de seguran√ßa do pod (PodSecurityPolicies) no Kubernetes.
* **Privil√©gios M√≠nimos:** Rodar o processo como non-root impede que, em caso de invas√£o da aplica√ß√£o, o atacante obtenha privil√©gios administrativos sobre o kernel do n√≥ hospedeiro.
* **ID 1001 vs appuser:** Em vez de apenas usar um nome como appuser, definir explicitamente o UID 1001 √© uma boa pr√°tica porque muitos sistemas de arquivos e ferramentas de seguran√ßa monitoram o ID num√©rico.
* **Minimiza√ß√£o de Ferramentas:** Ao usar --production, removemos ferramentas de build que poderiam ser exploradas por atacantes dentro do container.

### 5. Execu√ß√£o: Bin√°rio Direto vs Gerenciadores
* **Comando:** Foi definido o uso de `CMD ["node", "src/app.js"]`.
* **Sinais do Sistema:** O Node.js foi configurado como o processo principal (PID 1) para que possa receber sinais de termina√ß√£o do Kubernetes, como o `SIGTERM`. Gerenciadores como `npm` ou `yarn` costumam "encapsular" o processo, impedindo que os sinais cheguem ao Node, o que inviabilizaria um Graceful Shutdown (desligamento limpo).
* **Determinismo:** O uso do par√¢metro `--frozen-lockfile` no build garante que as vers√µes das depend√™ncias instaladas sejam exatamente as testadas, evitando desvios entre ambientes.

## ‚ò∏Ô∏è Tarefa 2: Deployment Kubernetes - Decis√µes T√©cnicas: Helm & Kubernetes

A arquitetura de deployment foi projetada para garantir alta disponibilidade, escalabilidade autom√°tica e isolamento de recursos, seguindo as melhores pr√°ticas de infraestrutura como c√≥digo.

### 1. Parametriza√ß√£o e Reutiliza√ß√£o (Helm)
* **Abstra√ß√£o via Values:** Todos os par√¢metros sens√≠veis e de configura√ß√£o (portas, caminhos de health check, limites de recursos) foram movidos para o arquivo `values.yaml`. Isso permite que o mesmo chart seja utilizado em diferentes ambientes apenas alterando o arquivo de valores, sem a necessidade de modificar os templates base.
* **Uso de Helpers:** Foi implementado o arquivo `_helpers.tpl` para gerenciar a nomenclatura dos recursos e labels de forma din√¢mica. O uso da fun√ß√£o `fullname` garante a unicidade dos nomes dentro do cluster, evitando colis√µes de recursos entre diferentes releases.

### 2. Alta Disponibilidade e Distribui√ß√£o (Topology Spread Constraints)
* **Estrat√©gia de Espalhamento:** Foi utilizada a funcionalidade de `topologySpreadConstraints` com `maxSkew: 1` e `topologyKey: kubernetes.io/hostname`. 
* **Justificativa:** Diferente de uma afinidade simples, o Spread Constraint garante matematicamente que as r√©plicas da aplica√ß√£o sejam distribu√≠das de forma equilibrada entre os n√≥s dispon√≠veis (`node-01` e `node-02`). O uso de `whenUnsatisfiable: DoNotSchedule` assegura que o cluster n√£o concentre pods em um √∫nico n√≥, mitigando o risco de downtime total em caso de falha de um host f√≠sico.

### 3. Resili√™ncia e Ciclo de Vida (PDB e Probes)
* **Pod Disruption Budget (PDB):** Foi implementado um PDB com `minAvailable: 1`. Esta configura√ß√£o √© vital para opera√ß√µes de SRE, pois impede que manuten√ß√µes automatizadas (como o dreno de um n√≥) desliguem todas as inst√¢ncias da aplica√ß√£o simultaneamente, garantindo que pelo menos 50% da capacidade esteja sempre ativa.
* **Health Checks Din√¢micos:** As Probes de `liveness` e `readiness` foram parametrizadas para validar a sa√∫de da aplica√ß√£o em tempo real. A separa√ß√£o entre liveness (rein√≠cio do container) e readiness (entrada no balanceador) garante que o tr√°fego s√≥ seja direcionado para pods que completaram seu processo de inicializa√ß√£o.

### 4. Escalabilidade Autom√°tica (HPA v2)
* **M√©tricas Combinadas:** O Horizontal Pod Autoscaler foi configurado para monitorar tanto CPU quanto Mem√≥ria simultaneamente.
* **Thresholds de Performance:** Foram definidos gatilhos de **70% para CPU** e **75% para Mem√≥ria**, conforme requisitos t√©cnicos do projeto. Esta abordagem h√≠brida protege a aplica√ß√£o contra gargalos de processamento e vazamentos de mem√≥ria (memory leaks), garantindo que o cluster escale horizontalmente de forma proativa antes da degrada√ß√£o da lat√™ncia.

### 5. Estrat√©gia de Deploy (Rolling Update)
* **Zero Downtime:** Foi configurada a estrat√©gia `RollingUpdate` com `maxUnavailable: 0`. Isso garante que o Kubernetes nunca remova uma vers√£o antiga da aplica√ß√£o sem antes ter uma nova vers√£o saud√°vel e pronta para receber tr√°fego, eliminando quedas de servi√ßo durante atualiza√ß√µes de vers√£o.
* **Justificativa:** Esta escolha garante que a capacidade total da aplica√ß√£o (2 r√©plicas) seja preservada durante todo o processo de atualiza√ß√£o. O Kubernetes √© for√ßado a instanciar um novo Pod saud√°vel antes de iniciar o encerramento de qualquer inst√¢ncia da vers√£o anterior, evitando gargalos de processamento durante janelas de deploy.

## üöÄ Tarefa 4: Pipeline CI/CD - Decis√µes T√©cnicas: CI/CD (GitHub Actions)

A automa√ß√£o do ciclo de vida da aplica√ß√£o foi implementada via GitHub Actions, focando em garantir a integridade do c√≥digo e a consist√™ncia dos deploys.

### 1. Pipeline de Integra√ß√£o Cont√≠nua (CI)
* **Build Multi-arquitetura:** O pipeline realiza o build da imagem Docker utilizando o contexto do Dockerfile otimizado, garantindo que apenas imagens que passaram nos testes de build sejam enviadas ao registro.
* **Versionamento de Imagem:** Foi adotada a estrat√©gia de versionamento via SHA do commit e a tag `latest` para o ambiente de staging, permitindo rastreabilidade total de qual vers√£o do c√≥digo est√° rodando em qual container.

### 2. Pipeline de Entrega Cont√≠nua (CD)
* **Helm Lint:** Antes de qualquer altera√ß√£o no cluster, o pipeline executa o `helm lint` para validar a sintaxe e as boas pr√°ticas dos templates do Chart, evitando falhas de deploy por erros de indenta√ß√£o ou l√≥gica de template.
* **Idempot√™ncia com Helm:** O deploy √© realizado atrav√©s do comando `helm upgrade --install`. Esta abordagem garante que o pipeline seja idempotente: se o release n√£o existir, ele √© criado; se j√° existir, √© atualizado com as novas configura√ß√µes e imagem.

### 3. Seguran√ßa e Portabilidade (Secrets Management)
* **Kubeconfig as a Secret:** A autentica√ß√£o com o cluster Kubernetes √© realizada atrav√©s da vari√°vel de ambiente `KUBECONFIG` armazenada nos GitHub Secrets. 
* **Justificativa:** Esta abordagem desacopla o pipeline da infraestrutura subjacente (iximiuz), permitindo que a estrat√©gia de deploy seja reutilizada em qualquer provedor de nuvem ou ambiente on-premises sem altera√ß√µes no c√≥digo. Al√©m disso, garante que credenciais sens√≠veis nunca fiquem expostas no reposit√≥rio.

### 4. Gest√£o de Imagens e Registro Externo (Docker Hub)
* **External Registry:** Foi adotado o Docker Hub como registro oficial de imagens da solu√ß√£o, em detrimento do registro ef√™mero local. 
* **Justificativa:** O uso de um registro externo garante a persist√™ncia dos artefatos de build independentemente da vida √∫til do cluster de teste. Isso facilita auditorias de seguran√ßa externas e permite que a imagem seja testada em m√∫ltiplos ambientes (Hybrid Cloud) sem necessidade de re-build.
* **Autentica√ß√£o Segura:** O acesso ao Docker Hub √© realizado via Personal Access Tokens (PAT) injetados como segredos no GitHub Actions, evitando a exposi√ß√£o de senhas globais da conta.

### 5. Portabilidade e Abstra√ß√£o do Pipeline
* **Generic Workflow:** O pipeline foi projetado para ser 100% agn√≥stico ao usu√°rio. Todas as refer√™ncias a nomes de registro, tags e contextos de infraestrutura foram movidas para GitHub Secrets.
* **Justificativa:** Isso permite que o projeto seja replicado por qualquer outro profissional apenas configurando seus pr√≥prios Segredos (Secrets), sem a necessidade de alterar uma √∫nica linha de c√≥digo nos arquivos YAML ou Helm. Esta abordagem segue o princ√≠pio de "Infrastructure as a Template".